Hand Gesture Recognition for Drone Control

Description:
This project aims to control a drone using hand gestures captured by a webcam. It employs deep learning techniques to recognize specific hand gestures, which are then translated into commands to control the drone's movement.

Teammates Details
M.Shiva Ashish - shivaashish669@gmail.com
M.Sumahas Reddy - sumahasreddymalkanelly@gmail.com
B.Sai Ram Reddy - 99210041507@klu.ac.in
B.Veeresh - 99210041692@klu.ac.in

Problem Solved:
Traditionally, controlling a drone requires specialized controllers or mobile applications, which may not always be convenient or intuitive. This project solves the problem of complex drone control interfaces by offering a more natural and accessible method through hand gestures. By leveraging computer vision and deep learning, users can interact with the drone effortlessly using simple hand movements.

Use Cases:
Drone Photography and Videography: Users can control the drone's movement and capture aerial shots without the need for complex controllers.

Search and Rescue Operations: In emergency situations, where traditional control methods may not be feasible, rescuers can use hand gestures to guide drones in search and rescue missions.

Surveillance and Security: Security personnel can utilize hand gesture control to maneuver drones for surveillance purposes, such as monitoring large areas or identifying potential threats.

Challenges:
Gesture Recognition Accuracy: Ensuring accurate detection and interpretation of hand gestures in various lighting conditions and backgrounds posed a significant challenge. Fine-tuning the deep learning models was crucial to improve recognition accuracy.

Real-time Processing: Processing video streams in real-time to detect and interpret hand gestures required optimization of algorithms and efficient utilization of computational resources.

User Interface Design: Designing an intuitive user interface to provide feedback on recognized gestures and drone movement presented a challenge. Clear and informative visualizations were essential for a seamless user experience.

Installation:
To run this project, you'll need to install the following dependencies:

OpenCV-Python: pip install opencv-python
MediaPipe: pip install mediapipe
TensorFlow: pip install tensorflow
Once the dependencies are installed, you can run the project using the 

command:
Copy code
python app.py
Usage
Run the Program: Execute the app.py script using the command provided above.

Camera Setup: Ensure your webcam is connected and properly configured.
Hand Gesture Recognition: Hold your hand in front of the camera and perform gestures to control the drone.
Show full palm to stop the drone.
Close the wrist to move the drone.
Rotate the hand to control the drone's rotation.
Exit: Press ESC to exit the program.
Contributing
Contributions are welcome! If you'd like to contribute to this project, feel free to submit a pull request.

Acknowledgments:
The hand detection and gesture recognition functionality are powered by the MediaPipe library.
Special thanks to the contributors of OpenCV, TensorFlow, and other dependencies used in this project.

Drive link of video and images:
[https://drive.google.com/drive/folders/1fWwP0g41FUxY765dnkfh9BdOxZHB9A2S?usp=drive_link](https://drive.google.com/file/d/1SwPiyzHbAk-qSg4tkOOUKcg4izFfAOIN/view?usp=sharing)https://drive.google.com/file/d/1SwPiyzHbAk-qSg4tkOOUKcg4izFfAOIN/view?usp=sharing



